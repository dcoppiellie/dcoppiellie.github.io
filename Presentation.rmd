---
title: "DDSAnalytics: Case Study 2"
author: "David Coppiellie"
date: "12/5/2019"
output: 
  ioslides_presentation: 
    smaller: yes
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
	message = FALSE,
	warning = FALSE)
####Load Libraries####
library(mlbench)
library(caret)
library(mlr)
library(tidyverse)
library(ggthemes)
library(gplots)
library(randomForest)
library(skimr)
library(corrplot) 
library(tidyverse)
library(cowplot)
library(GGally)
library(class)
library(e1071)
####Read Data####
Frito <- read.csv("C:\\Users\\Owner\\Desktop\\SMU DataScience\\DS6306\\Unit 14 and 15 Case Study 2\\CaseStudy2-data.csv",
                  strip.white = TRUE,
                  header = TRUE)
```
## Introduction 

- Project 2: DDSAnalytics
  - Developed and Presented By: David Coppiellie
<br>
<br>
Goals: 
  - Achieve 60% Sensitivity and 60% Specificty for Predictive Model on Attrittion
  - Develop Logistic Regression Model with RSME < $3,000
<br>
<br>
- Presentation: [Project 2](https://www.youtube.com/watch?v=XnEKwkAcfhg&t=10s)
- Project App: [Attrition App]()


## Review of Tasks

- Explore Employee Data Provided by DDSAnalytics
- Analyze Employee Data to Understand "Attrition"
- Provide a Predictive Model for "Attrition"
- Provide a Predictive Model for "MonthlyIncome"

## Exploration of Employeee Data

- Analyze Data Structure
- Analyze Correlations between Numeric Variables
- Analyze Correlations between Numeric and Categorical Variables
- Analyze Correlations between Categorical variables
- Eliminate Variables that Overfit a Model
- Compare k-NN and NaiveBayes Classifiers on "Attrition"
- Predict "MontlyIncome" with Linear Regression Analysis

## Data Structure
```{r}
str(Frito)
####Removing columns (Employee Count and Standard hours) with a single valuea####
####Eliminated variables EmployeeCount and StandardHours because they only contained a single value####
####Eliminated variable EmployeeNumber ID and Over18 as being irrelevant to any potential attrition analysis####
fl <- Frito %>% select(Age, Attrition, BusinessTravel,	DailyRate,	Department,
                       DistanceFromHome,	Education,	EducationField, EnvironmentSatisfaction,
                       Gender,	HourlyRate,	JobInvolvement,	JobLevel,	JobRole,	JobSatisfaction,
                       MaritalStatus,	MonthlyIncome,	MonthlyRate, NumCompaniesWorked,
                       OverTime,	PercentSalaryHike,	PerformanceRating,	RelationshipSatisfaction,
                       StockOptionLevel,	TotalWorkingYears,	TrainingTimesLastYear,	WorkLifeBalance,
                       YearsAtCompany,	YearsInCurrentRole,	YearsSinceLastPromotion,	YearsWithCurrManager)
```

## Numeric Variable Analysis
```{r}
fl %>% keep(is.numeric) %>% cor %>% corrplot("upper", addCoef.col = "black", number.digits = 2,
                                                            number.cex = 0.5, method="square",
                                                            order="hclust",
                                                            title="Variable Corr Heatmap",
                                                            tl.srt=45, tl.cex = 0.8)


correlator  <-  function(fl){
  Frito %>%
    keep(is.numeric) %>%
    tidyr::drop_na() %>%
    cor %>%
    corrplot("upper", addCoef.col = "white", number.digits = 2,
             number.cex = 0.5, method="square",
             order="hclust", title="Variable Corr Heatmap",
             tl.srt=45, tl.cex = 0.8)
}
####Eliminate variables with correlation with each other####
####I decided to use a 75% correlation threshold to eliminate similar variables that convey similar information.####
####With this, the assumed strongest indicator was chosen.####
####For example, the correlation between PercentSalaryHike and PerformanceRating show strong correlation.####
####PercentSalaryHike was selected as being more indicative of attrition as people are more likely to stay in a job where their salary increases.####
####Likewise, a person is less likely to leave a job as a high performer, except for a potential increase in pay at another job.####
##The PercentSalaryHike variable should represent this.  This same logic applies to JobLevel against MonthlyIncome, and MonthlyIncome against TotalWorkingYears, and TotalWorkingYears against JobLevel.####
####YearsAtCompany, and YearsinCurrentRole and YearsWithCurrManager also share a correlation.####
####YearsAtCompany was selected because the role of the manager and job responsibilities will be included in the time at the company.####
####RelationshipSatisfaction and MaritalStatus seem redudant, so relationship satisfaction was removed.####
####This same logic applies to OverTime and WorkLifeBalance, and WorkLifeBalance seems more inclusive.####
####Additonally, DailyRate, HourlyRate, and MonthlyRate would seem to be included in the MonthlyIncome calculations, and are therefore redundant with TotalWorkingYears.####
fl2 <- Frito %>% select(Age, Attrition, BusinessTravel,	Department,
                       DistanceFromHome,	Education,	EducationField, EnvironmentSatisfaction,
                       Gender,	JobInvolvement,	MonthlyIncome, JobRole,	JobSatisfaction,
                       MaritalStatus, NumCompaniesWorked,
                       PercentSalaryHike,
                       StockOptionLevel,	TrainingTimesLastYear,	WorkLifeBalance,
                       YearsAtCompany,	YearsSinceLastPromotion)
```

## Numeric and Categorical Variable Analysis
```{r}
####Numeric:Catergorical variable elimination####
#####Convert categorical variables to factors####
fl2$Attrition <- as.factor(fl2$Attrition)
fl2$BusinessTravel <- as.factor(fl2$BusinessTravel)
fl2$Department <- as.factor(fl2$Department)
fl2$Education <- as.factor(fl2$Education)
fl2$EducationField <- as.factor(fl2$EducationField)
fl2$EnvironmentSatisfaction <- as.factor(fl2$EnvironmentSatisfaction)
fl2$Gender <- as.factor(fl2$Gender)
fl2$JobInvolvement <- as.factor(fl2$JobInvolvement)
fl2$JobRole <- as.factor(fl2$JobRole)
fl2$JobSatisfaction <- as.factor(fl2$JobSatisfaction)
fl2$MaritalStatus <- as.factor(fl2$MaritalStatus)
fl2$WorkLifeBalance <- as.factor(fl2$WorkLifeBalance)
####Target Variable####
target <- "Attrition"
####Explanatory variables####
numvars <- fl2 %>% keep(is.numeric) %>% colnames
numplot <- function(df, explan, resp) {
  ggplot(data = df) + geom_density(aes_string(x = explan, fill = resp), alpha = 0.5)
}
plotlist <- lapply(numvars, function(x) numplot(fl2, x, "Attrition"))
plot_grid(plotlist = plotlist)
####Based on above analysis, more variables may be elminated.#### 
####For example, Age, PercentSalaryHike, NumCompaniesWorked, YearsSinceLastPromotion, TrainingTimesLastYear, YearsAtCompany.####
fl3 <- Frito %>% select(Attrition, BusinessTravel,	Department,
                        DistanceFromHome,	Education,	EducationField, EnvironmentSatisfaction,
                        Gender,	JobInvolvement,	MonthlyIncome, JobRole,	JobSatisfaction,
                        MaritalStatus,
                        StockOptionLevel,	WorkLifeBalance)
```

## Categorical Variable Analysis
```{r echo = FALSE}
####Categorical:Categorical variable Elmination####
####Target variable####
target <- "Attrition"
####Explanatory Variables####
expls <- fl3 %>% keep(is.factor) %>% colnames
catplot <- function(df, x,y){
  ggplot(data = df, aes_string(x = x, fill = y)) + 
    geom_bar(position = "fill", alpha = 0.9) + 
    coord_flip()
}
plotlist2 <- lapply(expls, function(x) catplot(fl3, x, target))
plot_grid(plotlist = plotlist2)
####At this point, an assumption will be made that EnvironmentSatisfacation and JobSatisfaction are redundant, and JobStatisfaction will be kept.####
####Same assumption applies to BusinessTravel, WorkLifeBalance and JobSatisfacation, and JobSatisfaction will be kept.####
####Another assumption that was applied was that Education, and JobRole can be accounted for in JobLevel.####
fl4 <- Frito %>% select(Attrition, DistanceFromHome, JobInvolvement, 
                        MonthlyIncome, JobSatisfaction, MaritalStatus,
                        StockOptionLevel)
```

## Check for Overfit Variables
```{r}
fl4 %>% ggpairs(aes(color= Attrition))
####From ggpairs plot, JobInvolvement, JobSatisfaction, and MaritalStatus appear to have the same or similar means, and can be eliminated.####
flfinal <- Frito %>% select(Attrition, DistanceFromHome, MonthlyIncome, StockOptionLevel)
```

## k-NN Classifications for "Attrition"
```{r}
#####Convert categorical variables to factors###
flfinal$Attrition <- as.factor(flfinal$Attrition)
#####Convert StockOptionLevel to numeric###
flfinal$StockOptionLevel <- as.numeric(flfinal$StockOptionLevel)
####Spit Data set into a training Data set and a testing dataset with a 75/25 split.####
####Additionally, the model was trained across 100 seeds to ensure greater accuracy.####
sp = 0.75  
for (seed in 1:100)
{
  set.seed(seed)
  TrainingRows = sample(1:dim(flfinal)[1],round(sp * dim(flfinal)[1])) # Calculate Training Rows
  fl_train = flfinal[TrainingRows,]  # Split into 2 seperate data frames. Include Training Rows
  fl_test = flfinal[-TrainingRows,]  # Exclude Training Rows (Testing Rows)
  classifications = knn(fl_train[,c(2,3,4)], fl_test[,c(2,3,4)],
                        fl_train$Attrition, k=7, prob = TRUE)
  table(fl_test$Attrition, classifications)
  cm = confusionMatrix(table(fl_test$Attrition, classifications))
}
cm
###Attrition prediction#####
####Read Data####
attrition <- read.csv("C:\\Users\\Owner\\Desktop\\SMU DataScience\\DS6306\\Unit 14 and 15 Case Study 2\\CaseStudy2CompSet No Attrition.csv", 
                      strip.white = TRUE,
                      header = TRUE)
####Predict Test Data from Model####
attrition$Attrition <- knn(fl_train[,c(2,3,4)], attrition[,c("DistanceFromHome", "MonthlyIncome", "StockOptionLevel")], fl_train$Attrition, k=7, prob = TRUE)
####Output Data####
write.csv(attrition, "C:\\Users\\Owner\\Desktop\\SMU DataScience\\DS6306\\Unit 14 and 15 Case Study 2\\Case2PredictionsCoppiellie Attrition.csv")
```

## NaiveBayes Model for "Attrition"
```{r}
####Naive-Bayes Model######
####With an average of 100 Train/Test splits, the NaiveBayes Model provided no greater sensivity or accuracy than the k-NN Model, which was chosen as the predictor for the Test data.####
for (seed in 1:100)
{
  set.seed(seed)
  trainIndices = sample(seq(1:length(flfinal$Attrition)),round(.7*length(flfinal$Attrition)))
  trainfl = flfinal[trainIndices,]
  testfl = flfinal[-trainIndices,]
  model = naiveBayes(trainfl[,c("DistanceFromHome", "MonthlyIncome","StockOptionLevel")],factor(trainfl$Attrition, labels = c("No", "Yes")))
  CM = confusionMatrix(table(factor(testfl$Attrition, labels = c("No", "Yes")),predict(model,testfl[,c("DistanceFromHome", "MonthlyIncome","StockOptionLevel")])))
}
CM
```

## Predicting "MontlyIncome" with Linear Regression
```{r}
####Linear Regression####
flfinal2 <- Frito %>% select(Attrition, DistanceFromHome, MonthlyIncome, StockOptionLevel, JobLevel, MaritalStatus)
####Linear Regression Predictive Model####
trainIndices = sample(seq(1:length(flfinal2$MonthlyIncome)),round(.7*length(flfinal2$MonthlyIncome)))
trainfl2 = flfinal2[trainIndices,]
testfl2 = flfinal2[-trainIndices,]
fit2 <- lm(MonthlyIncome~DistanceFromHome + StockOptionLevel + Attrition + JobLevel, data=flfinal2)
monthinc <- predict(fit2, testfl2)
###Find p-values, Adjusted R^2 values, and other summary statisitics.####
summary(fit2)
confint(fit2)
####Salary Predictions####
####Read Data####
nsalary <- read.csv("C:\\Users\\Owner\\Desktop\\SMU DataScience\\DS6306\\Unit 14 and 15 Case Study 2\\CaseStudy2CompSet No Salary.csv", 
                   strip.white = TRUE,
                   header = TRUE)
####Predict Test Data from Model####
nsalary$salary <- predict(fit2, nsalary)
####Output Data####
write.csv(nsalary, "C:\\Users\\Owner\\Desktop\\SMU DataScience\\DS6306\\Unit 14 and 15 Case Study 2\\Case2PredictionsCoppiellie Salary.csv")
```

## Conclusion
- Employee Data Analyzed for Predictive Variables
- k-NN computed a better Predictive Model for "Attrition" than NaiveBayes
  - Sensitivity: 85.4%
  - Specificity: 60%
- Linear Regression computed an accurate Predictive Model for "MonthlyIncome"
  - DistanceFromHome statistically significant (p-value = 0.08)
  - RSME = $1410
  - Adjusted R-squared: 0.9059






